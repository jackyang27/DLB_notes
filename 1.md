5.8 无监督学习

- 学习对数据更好的表达
    - 遵循约束前提下，更简单或更容易使用的表达方式
    - 低维表达
    - 稀疏表达（高维）
    - 独立表达（解耦）
    - 低维表达经常伴随着解耦，因为它们都需要去除冗余
    - 几个样例算法
        - PCA
            - 是对原数据进行了线性变换，变换后的矩阵有一个 diagonal covariance matrix
            - 因为只是线性变换，所以可能在深度学习的衬托下显得不那么强大了？
            - 为了更好的解耦，还要去除变量间的非线性关系
            - 用到了奇异值分解 SVD
            - 知乎提问：新人弱鸡求教：读花书读到 PCA 那里，有一个疑问：在深度学习中，MLP 是否可以代替 PCA，PCA 的用武之地是否被深度神经网络侵占了？
        - K-means
            - 学到一个词 sparse representations。one-hot 是其极端形式。
                - one-hot：特征明确（其实意味着特征解耦呗）
            - 另一个词 distributed representation
            - K-means 有两个步骤反复循环，直到收敛。
                - 第一，根据各个 cluster 的中心来分配训练数据
                - 第二，根据每个 cluster 的训练数据计算更新 其中心
            - K-means 的缺点：需要预先知道分成几个 cluster；就算让模型自己判断分成几个 cluster，模型也不知道要分的有多细。
            - 另外 onehot 也不能保留相似度，不如 distributed representations 好。
            - p 150
- 以后的算法也遵循这三个准则